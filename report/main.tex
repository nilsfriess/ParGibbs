\documentclass[
fontsize=11pt,
paper=a4,
numbers=noenddot
]{scrartcl}

\usepackage[bottom=3cm, top=3cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}

\usepackage[british]{babel}

\usepackage{csquotes}

\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}

\usepackage{hyperref}  

\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}

\newtheorem{theorem}{Theorem}

\usepackage{todonotes}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwData}{Input}


\addtokomafont{disposition}{\rmfamily}

\title{Parallel Multigrid Monte Carlo\\
{\normalsize Software practical report}}
\date{\today}
\author{Nils Friess}

\begin{document} 
\maketitle

\begin{abstract}
    This report presents a parallel implementation of the Multigrid Monte Carlo (MGMC) method for sampling from Gaussian distributions with large and sparse precision matrix on distributed memory machines. MGMC is a stochastic counterpart of the classic geometric Multigrid method where the smoothers are replaced by certain random samplers. We give a brief theoretical introduction to the method before discussing our implementation based on the \emph{Portable and Extensible Toolkit for Scientific computing (PETSc)}. We provide several numerical examples to study both the general characteristics of the MGMC method and the scaling of the parallel implementation.
\end{abstract}

\section{Introduction}
Efficiently generating samples from high-dimensional Gaussian distributions is an important task in many computational sciences but remains computationally challenging when the problem becomes so large that the data does not fit in the memory of a single computing node or if the computation should be sped up by distributing the work among multiple processors or computers. The go--to method for generating Gaussian samples uses a Cholesky factorisation of the precision matrix (the inverse covariance matrix) but due to the computational cost of $\mathcal{O}(n^2 b)$ (where $n$ is the number of rows of the precision matrix and $b$ its bandwidth) it becomes prohibitive for large problems. Iterative methods such as the Gibbs sampler offer a cost per sample that scales better, but the generated samples are statistically correlated so that in certain situations the cost per \emph{statistically independent} sample can again become so large that it renders the method practically useless.

In this report we present a parallel implementation of the \emph{Multigrid Monte Carlo (MGMC)} method~\cite{goodmansokal} based on the \emph{Portable and Extensible  Toolkit for Scientific computing (PETSc)}~\cite{petsc-web-page,petsc-user-ref}. MGMC scales optimal with respect to the size of the matrix and typically only 2--5 samples are required to obtain a statistically independent sample. 

In the next section we give a brief introduction to the MGMC method. In the third section we then give an overview of our implementation. Since the MGMC method is in a certain sense a stochastic version of the classic (geometric) Multigrid algorithm, large parts of the implementation can reuse parallel data structures and algorithms provided by PETSc (e.g., parallel matrix formats, or the parallel assembly and application of grid-transfer operators) so that we only briefly comment on those aspects of the implementation. Our focus is the parallelisation of the Gibbs sampler that replace the smoothers in the classic Multgrid method. In the last section we study the parallel scaling of the method for different problems and discuss open problems and possible research directions.

\section{Multigrid Monte Carlo}
MGMC was first introduced in the context of simulating lattice field theories as a means to reduce or even eliminate the phenomenon of \emph{critical slowing down}: For classical algorithms, such as the \emph{heat bath} algorithm, the autocorrelation time $\tau$ (the time that is necessary to obtain a new useful grid configuration during the simulation) typically behaves as $\tau \sim h^z$ where $h$ is the grid spacing and usually $z \approx 2$. Thus, as the continuum limit $h \rightarrow 0$ is approached, the computer time increases drastically, rendering these algorithms practically useless. As already described in the original paper~\cite{goodmansokal}, the same holds true when these algorithms are used to generated Gaussian samples (since this simply corresponds to a specific type of Hamiltonian operator that is used to describe the physical theory). In this case, the \emph{heat bath} algorithm is a Gibbs sampler and the autocorrelation time roughly corresponds to the number of samples that need to be generated to obtain one statistically independent sample.

In essence, the MGMC algorithm corresponds to the classic (geometric) multigrid algorithm~\cite{hackbuschMultiGridMethodsApplications1985}, whereby the \emph{pre- and postsmoothers} are replaced by \emph{pre- and postsamplers} (which we also refer to as random smoothers to emphasise the connection to classic Multigrid) and the direct solver on the coarsest level is replaced by a random sampler. More precisely, for every Multigrid method with a smoother based on a convergent matrix splitting there exists a corresponding MGMC method (if we fix the coarse sampler). This correspondence follows from a general equivalence between stationary iterative methods and so called generalised Gibbs samplers~\cite{foxparker}. In the case of the Gauss-Seidel method, for example, the corresponding sampler is the classical component-wise Gibbs sampler.

The remaining components of the Multigrid method (in particular, the prolongation and restriction operations) are used in the same way in the MGMC method. An implementation can thus reuse parts of a given Multigrid implementation and for our parallel implementation we can leverage PETSc's optimised parallel implementations of these components. For the coarse sampler, we either use a few iterations of a Gibbs sampler or we generate the samples using a Cholesky factorisation of the coarse matrix. We used the 

\section{Parallel Gibbs sampling}
To define the MGMC sampler as an analogue of the Multigrid method, Goodman and Sokal~\cite{goodmansokal} exploited a close connection between the Gauß-Seidel iterative method and the Gibbs sampler. Fox and Parker showed that this connection is only a special case of a broader correspondence between iterative methods based on matrix splittings and so called generalised Gibbs samplers.

Recall that the target distribution is $\pi \coloneqq \mathcal{N}(A^{-1}f, A^{-1})$ with $(A, f) \in \mathbb{R}^{n \times n} \times \mathbb{R}^n$ and $A$ symmetric positive definite. Let now $A = M - N$ be a matrix splitting with $M$ invertible, and let $y^{(0)} \in \mathbb{R}^n$ be arbitrary. Then, the stochastic iteration
\begin{equation}
    \label{eq:stochasticit}
    M y^{(k+1)} = N y^{(k)} + c^{(k)}\,,\qquad k = 0,1,2, \dotsc,
\end{equation}
with $c^{(k)} \sim \mathcal{N}(f, M^T + N)$, converges in distribution to $\pi$ (see~\cite[Thm.\ 2 and Cor.\ 4]{foxparker}). It is evident that the practicality of a sampler derived from this idea depends on the form of $M^T + N$. If $A$ and $M^T + N$ have essentially the same structure, then this approach merely moves the problem of sampling from $\mathcal{N}(0, A)$ to that of sampling from $\mathcal{N}(0, M^T + N)$. It turns out that the choice $M = D + L$, $N = -L^T$ lies at a sweet spot since $M^T + N = D$ and thus sampling from $\mathcal{N}(f, M^T + N)$ is easy, and solving the linear system in~\eqref{eq:stochasticit} is also simple (see below). Here, $L$ is the strictly lower triangular part of $A$ and $D$ is the diagonal. In the deterministic case, this matrix splitting defines a Gauß-Seidel solver; in the stochastic case, this is a Gibbs sampler. Using $M = \frac{1}{\omega} D + L$, $N = \frac{1-\omega}{\omega} D - L^T$, for $\omega \in (0,2)$, offers the same advantages and corresponds to the method of successive over-relaxation (SOR) in the deterministic case and we refer to the stochastic variant as SOR-Gibbs. Since this reduces to a standard Gibbs sampler for $\omega = 1$, we now study SOR-Gibbs in detail.

Let us first write out the iteration~\eqref{eq:stochasticit}. Inserting $M$ and $N$ gives
\begin{equation*}
    \left(\frac{1}{\omega} D + L\right) y^{(k+1)} = \left(\frac{1-\omega}{\omega} D - L^T\right) y^{(k)} + c^{(k)}\,.
\end{equation*}
Multiplying both sides by $\omega D^{-1}$ and rearranging terms then gives
\begin{equation*}
    y^{(k+1)} = (1 - \omega) y^{(k)} - \omega D^{-1} (L y^{(k+1)} + L^T y^{(k)}) + \omega D^{-1} c^{(k)}\,.
\end{equation*}
Component-wise we have
\begin{equation*}
    y^{(k+1)}_j = (1-\omega) y^{(k)}_j - \frac{\omega}{a_{jj}} \left(
        \sum_{m=1}^{j-1} a_{mj} y^{(k+1)}_m + \sum_{m=j+1}^n a_{mj} y^{(k)}_m
    \right) + \frac{\omega}{a_{jj}} c^{(k)}_j \,.
\end{equation*}
Since the first sum only contains entries of $y^{(k+1)}$ with index \emph{smaller} than $j$, and the second sum only contains entries of $y^{(k)}$ with index \emph{larger} than $j$, one can update the sample in-place, giving Algorithm~\ref{alg:gibbs-sor}. To compute $\omega D^{-1} c^{(k)}$ we first note that since $c^{(k)} \sim \mathcal{N}(f, (2-\omega)/\omega D)$ we have $\omega D^{-1} c^{(k)} \sim \mathcal{N}(\omega D^{-1}f, \omega(2-\omega)D^{-1})$ which we compute by first sampling $z \sim \mathcal{N}(0, I)$ and transforming the result linearly to obtain the correct mean and covariance (see Line 2 of Algorithm~\ref{alg:gibbs-sor}).

\begin{algorithm}[htpb]
    \SetAlgoLined
    \KwData{Precision matrix $A$, vector $f$, SOR parameter $\omega$, number of iterations $K$, initial sample $y$}
    \KwResult{Sample $y$}
    \For{$k \leftarrow 1$ \KwTo $K$}{
      Sample $z \sim \mathcal{N}(0, I)$ \;
      Transform $z \leftarrow \sqrt{\omega(2 - \omega)} D^{-1/2} z + \omega D^{-1} f$ \;
      Update $y_j \leftarrow (1-\omega) y_j - \frac{\omega}{a_{jj}} \left(
            \sum_{m=1}^{j-1} a_{mj} y_m + \sum_{m=j+1}^n a_{mj} y_m
        \right) + z$ \;
    }
    \caption{SOR-Gibbs sampler}\label{alg:gibbs-sor}
  \end{algorithm}


\section{Numerical results}

\section{Conclusion}

\section{Notes}
\subsection{Cholesky sampling from Gaussian distributions with given precision matrix}
Suppose we want to generate samples from the distribution $\pi = \mathcal{N}(A^{-1} f, A^{-1})$. Let $LL^T = A$ be a Cholesky decomposition of the precision matrix $A$. Samples from $\pi$ can then be generated as follows:
\begin{enumerate}
    \item Draw a sample $\xi \sim \mathcal{N}(0,I)$.
    \item Forward-solve $L g = f$ for $g$.
    \item Backward-solve $L^T x = \xi + g$.
    \item Return the new sample $x$.
\end{enumerate}
Indeed, we then have $x = L^{-T} \xi + L^{-T} g = L^{-T} \xi + {(LL^T)}^{-1} f =  L^{-T} \xi + A^{-1} f$ and by the properties of the normal distribution it follows that $x \sim \mathcal{N}(A^{-1} f, L^{-T}L^{-1}) = \mathcal{N}(A^{-1} f, A^{-1})$ as claimed.

\subsection{Finite Difference discretisation of the shifted Laplace problem}
Consider the following PDE
\begin{align*}
    -u_{xx}(x,y) - u_{yy}(x,y) + \kappa^2 u(x,y) &= f \qquad \text{for $0 < x,y < 1$} \\
    u(x,y) &= 0 \qquad \text{for $x = 0, x = 1, y = 0, y = 1$.}
\end{align*}
Using a classic 5-point stencil, we obtain the discrete equations
\begin{equation*}
    -\frac{1}{h^2}\left(
        u_{i+1,j} + u_{i,j+1} - 4u_{i,j} + u_{i,j-1} + u_{i-1,j}
    \right)
    + \kappa^2 u_{i,j} = 0\,,
\end{equation*}
for $i,j = 1,\dotsc,n-1$ and (due to the Dirichlet boundary condition)
\begin{equation*}
    u_{0,j} = u_{i,0} = u_{n,j} = u_{i,n} = 0\,, \qquad\text{for $i,j = 0,\dotsc,n$.} 
\end{equation*}

\subsection*{Mat\'ern Covariance functions and shifted Laplace PDEs}
Consider the stochastic partial differential equation
\begin{equation*}
    \tau {(\kappa^2 - \Delta)}^{\alpha / 2} u = \mathcal{W}
\end{equation*}
posed on the whole of $\mathbb{R}^d$. Whittle has shown that the stationary solutions of this SPDE have Mat\'ern covariance with $\nu = \alpha - d/2$. Using a close connection between those solutions and the inner product of an associated reproducing kernel Hilbert space, one can show that the corresponding precision operator is
\begin{equation*}
    \mathcal{A}_\alpha \coloneqq \tau^2 {(\kappa^2 - \Delta)}^\alpha\,,
\end{equation*}
see~\cite[Sec.\ 2.2]{lindgrenSPDEApproachGaussian2022}.


\printbibliography

\end{document}