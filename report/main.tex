\documentclass[
fontsize=11pt,
paper=a4,
numbers=noenddot
]{scrartcl}

\usepackage[bottom=3cm, top=3cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}

\usepackage[british]{babel}

\usepackage{subcaption}

\usepackage{booktabs}

\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}

\usepackage[hidelinks]{hyperref}   

\usepackage{mathtools} 
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{tikz}

\newtheorem{theorem}{Theorem}

\usepackage{todonotes}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwData}{Input}

\usepackage{minted}
\usepackage{csquotes}

\addtokomafont{disposition}{\rmfamily}

\title{\vspace{-1cm}Parallel Multigrid Monte Carlo\\
{\normalsize\normalfont\emph{Software practical report}}}
\date{\normalsize Submitted: \today}
\author{\normalsize Author: Nils Friess\\
\normalsize Supervisor: Prof. Robert Scheichl}

\begin{document}
\maketitle

\begin{abstract}
    This report presents a parallel implementation of the Multigrid Monte Carlo (MGMC) method for sampling from Gaussian random fields on distributed memory machines. We give a brief theoretical introduction to the method before discussing our implementation based on the Portable and Extensible Toolkit for Scientific computing (PETSc). We provide several numerical examples to study the performance and scaling of the parallel implementation.
\end{abstract}

\section{Introduction}
Efficiently generating samples from high-dimensional Gaussian distributions is an important task in many computational sciences but remains computationally challenging when the problem becomes very large, or when the computation should be sped up by distributing the work among multiple processors. The go--to method for generating Gaussian samples uses a Cholesky factorisation of the covariance matrix or the precision matrix (the inverse covariance matrix) and although the computational cost of the decomposition is $\mathcal{O}(nb^2)$, where $n$ is the number of rows of the covariance/precision matrix and $b$ its bandwidth, it can become prohibitively expensive if the bandwidth is high and cannot be reduced sufficiently using a permutation of the given matrix~\cite{golubvanloan,rue2001fast,foxparker}. Further, the additional memory required to store the Cholesky factor can become problematic for large problems. Iterative methods such as the Gibbs sampler are a popular alternative, but the generated samples are statistically correlated so that in certain situations the cost per statistically independent sample can become so large that it renders the method practically useless~\cite{foxparker,kazashimuellerscheichl}. In this report we instead consider the Multigrid Monte Carlo (MGMC) method~\cite{goodmansokal} which overcomes the aforementioned limitations. We present a parallel implementation based on the Portable and Extensible Toolkit for Scientific computing (PETSc)~\cite{petsc-web-page,petsc-user-ref}.

The report is structured as follows. In the next section we provide a brief introduction to the MGMC method before giving an overview of our parallel implementation in the third section. Since the MGMC method is in a certain sense a stochastic version of the classic (geometric) multigrid algorithm, most components of the algorithm can be parallelised in the same way as in classic multigrid. However, certain parts---in particular the parallel Gibbs samplers that replace the pre- and post-smoothers---have to be implemented from scratch. We will therefore only briefly discuss the parts that are already implemented in PETSc and focus on the  parallelisation of the Gibbs samplers. In the last section we present three numerical examples to study the parallel performance of our implementation. We conclude with a discussion of open problems and possible research directions.

\section{Multigrid Monte Carlo}
We consider the problem of generating samples from discrete Gaussian random fields with given precision matrix. This includes sampling from Gaussian Markov Random Fields~\cite{rue2005gaussian}, or discretisations of continuous random fields on regular lattices or unstructured grids in $\mathbb{R}^2$ and $\mathbb{R}^3$. Let $\pi\coloneqq\mathcal{N}(A^{-1}f, A^{-1})$ be the target distribution, with $f \in \mathbb{R}^n$ and $A \in \mathbb{R}^{n \times n}$ symmetric positive definite. We assume that $A$ is large but sparse. The standard method to sample from $\pi$ is based on the Cholesky decomposition of $A$. To this end, one computes a Cholesky factorisation $A = LL^T$, with $L \in \mathbb{R}^{n \times n}$ lower triangular, performs the forward-solve $L g = f$, followed by the backward-solve $L^T x = \xi + g$, where $\xi \sim \mathcal{N}(0,I)$. One readily checks that then $x \sim \pi$. If $A$ has bandwidth $b$, the cost of generating one sample is $\mathcal{O}(2nb)$, and the cost of computing the Cholesky factor is given by $\mathcal{O}(nb^2)$~\cite{golubvanloan}. This is feasible if $b$ is sufficiently small or if a permutation matrix $P$ exists that reduces the bandwidth of $PAP^T$ sufficiently. However, in many practically relevant cases this is not possible, sometimes leading to a computational cost of $\mathcal{O}(n^{7/3})$ to compute the Cholesky factor~\cite{foxparker}.

Generalised Gibbs samplers based on matrix splittings~\cite{foxparker} (including the standard Gibbs sampler~\cite{gemangeman}) are popular alternatives in these cases since their computational cost is independent of the bandwidth. Let $A = M - N$ be a matrix splitting with $M$ invertible, and let $y^{(0)} \in \mathbb{R}^n$ be arbitrary. A generalised Gibbs sampler is given by the stochastic iteration
\begin{equation}
    \label{eq:stochasticit}
    M y^{(k+1)} = N y^{(k)} + c^{(k)}\,,\qquad k = 0,1,2, \dotsc,
\end{equation}
with $c^{(k)} \sim \mathcal{N}(f, M^T + N)$. Upon replacing $c^{(k)}$ by $b \in \mathbb{R}^n$ for all $k \in \mathbb{N}_0$, one obtains instead a (deterministic) iterative method for solving the linear system $Ay=b$. By exploiting this similarity, Fox and Parker~\cite{foxparker} have shown that the samples generated by~\eqref{eq:stochasticit} converge in distribution to $\pi$ if, and only if, the corresponding stationary iterative solver converges.

The choice $M = D + L$, $N = -L^T$, where $L$ is the strictly lower triangular part of $A$ and $D$ its diagonal, corresponds to the standard Gibbs sampler, and the deterministic counterpart is the Gauss-Seidel method. Inserting $M$ and $N$ in~\eqref{eq:stochasticit} gives
\begin{equation*}
    \left(D + L\right) y^{(k+1)} = - L^T y^{(k)} + c^{(k)}\,,
\end{equation*}
with $c^{(k)} \sim \mathcal{N}(f,D)$. Multiplying both sides by $D^{-1}$ and rearranging terms then gives
\begin{equation*}
    y^{(k+1)} = - D^{-1} (L y^{(k+1)} + L^T y^{(k)}) + D^{-1} c^{(k)}\,.
\end{equation*}
Component-wise we thus have
\begin{equation}
    \label{eq:gibbs_comp}
    y^{(k+1)}_j = - \frac{1}{a_{jj}} \left(
      \sum_{m=1}^{j-1} a_{mj} y^{(k+1)}_m + \sum_{m=j+1}^n a_{mj} y^{(k)}_m
    \right) + \frac{1}{a_{jj}} c^{(k)}_j \,,\quad \text{for $j=1,\dotsc,n$.}
\end{equation}
Modifications of the standard Gibbs sampler can be obtained by changing the order in which the components are updated. We call the variant as defined above a forward Gibbs sweep; reversing the order leads to a backward Gibbs sweep. A forward sweep followed by a backward sweep is referred to as a symmetric sweep.  

If $A$ has $\mathcal{O}(n)$ non-zero entries, the cost of generating one sample using a Gibbs sampler is about $\mathcal{O}(2n)$. However, since the generated samples are highly correlated, multiple iterations are required to obtain one statistically independent sample. If, for instance, $\pi$ is a Gaussian random field discretised on a regular lattice with grid spacing $h$, in certain cases the number of iterations required to obtain a useful sample depends inversely on $h$ so that the computer time increases drastically as $h \rightarrow 0$~\cite{goodmansokal,kazashimuellerscheichl}.

To overcome this issue, Goodman and Sokal~\cite{goodmansokal} have borrowed ideas from multigrid methods to define a hierarchical sampling algorithm, called Multigrid Monte Carlo (MGMC). The general structure is essentially the same as the classic geometric multigrid algorithm~\cite{hackbuschMultiGridMethodsApplications1985}. Suppose we are given a hierarchy of grids. Given a sample on the finest level of the hierarchy, random coarse grid corrections are generated recursively using Gibbs samplers to update the fine grid sample. The Gibbs samplers play the role of the smoothers in classic multigrid and to emphasise the connection, we refer to them as random smoothers in this context. To transfer samples between grids, prolongation and restriction operators are introduced in the same way as in classic  geometric multigrid. If we label the grids from $\ell = 0$ (the coarsest level) to $\ell = L$ (the finest level), and let $n_\ell$ be the number of vertices of grid $\ell$, then we denote by $I_{\ell-1}^\ell \in \mathbb{R}^{n_\ell \times n_{\ell - 1}}$ the so-called prolongation matrix. Its transpose $I_{\ell}^{\ell-1} \coloneqq {(I_{\ell-1}^\ell)}^T \in \mathbb{R}^{n_{\ell - 1} \times n_{\ell}}$ is called the restriction matrix (for details on how these matrices are defined we refer to the multigrid literature, e.g.,~\cite{hackbuschMultiGridMethodsApplications1985}). Given the fine level precision matrix $A_L \coloneqq A$, the coarse level precision matrices are defined by Galerkin projection
\begin{equation}
  \label{eq:galerkin}
    A_{\ell-1} = I_\ell^{\ell - 1} A_\ell I_{\ell-1}^\ell \in \mathbb{R}^{n_{\ell-1} \times n_{\ell-1}}\,, \quad \text{for\quad$\ell = 1, \dotsc, L$.}
\end{equation}
The matrices $A_\ell$, $\ell = 0, \dotsc, L$, are then used as the precision matrices for the Gibbs samplers on each level. The full MGMC algorithm is summarised in Algorithm~\ref{alg:mgmc}.

\begin{algorithm}
    \DontPrintSemicolon

    \SetKwFunction{Fmgmc}{mgmc}
    \SetKwFunction{Fcoarsesampler}{coarseSampler}
    \SetKwFunction{Frandomsmoother}{randomSmoother}

    \SetKwProg{Fn}{Function}{:}{}

    \SetAlgoLined
    \Fn{\Fmgmc{$y$, $f$, $\ell$}}{
        \If{$\ell == 0$}{
            $y \gets$\Fcoarsesampler{$f$}
        }
        \Else{
            $y \gets$\Frandomsmoother{$A_\ell$, $y$, $f$} \tcp*{Random pre-smoothing}
            $r \gets f - A_\ell y$\tcp*{Compute residual}
            $f^\prime \gets I_{\ell}^{\ell - 1} r$ \tcp*{Restrict residual}
            $v \gets 0$ \tcp*{Initialise coarse grid correction}
            \For{$c =1$ to $\gamma$} {
                $v \gets$ \Fmgmc{$v$, $f^\prime$, $\ell - 1$}
            }
            $y \gets y + I_{\ell-1}^{\ell} v$\tcp*{Prolongate-add the correction}
            $y \gets$\Frandomsmoother{$A_\ell$, $y$, $f$} \tcp*{Random post-smoothing}
        }
        \Return{$y$}
    }

    \caption{Multigrid Monte Carlo}\label{alg:mgmc}
\end{algorithm}

Given a sample $y \in \mathbb{R}^{n_L}$, a new sample $y^\prime \in \mathbb{R}^{n_L}$ is obtained by calling $y^\prime =$ \texttt{mgmc(}$y,\, f,\, L$\texttt{)}. The cycle parameter $\gamma$ appearing in line 10 dictates how often the coarser grid is visited. Typically, either $\gamma = 1$ (the so-called V-cycle) or $\gamma = 2$ (the so-called W-cycle) is used. As in classic multigrid, it can be beneficial to run a few iterations of the random smoothers. To keep the presentation simple, this is not included in the algorithm above (but can be configured in our implementation). On the coarsest level one can either use a Gibbs sampler as on the other levels, or a Cholesky sampler (if the coarse problem size is sufficiently small).

As mentioned in the original paper~\cite{goodmansokal} and rigorously proved by Kazashi et al.~\cite{kazashimuellerscheichl} a sequence of samples $y^{(0)}, y^{(1)}, \dotsc$ generated iteratively by 
\begin{equation*}
    y^{(k+1)} = \texttt{mgmc(}y^{(k)},\, f,\, L\texttt{)},\qquad k = 0,1,\dotsc,
\end{equation*}
with $y^{(0)}$ drawn from an arbitrary Gaussian distribution, converges to $\mathcal{N}(A^{-1}f, A^{-1})$ in distribution with a rate that is independent of the grid spacing of the finest level (under suitable conditions). Moreover, the cost of one call to the method is optimal: if the number of smoothing steps, the cycle parameter, and the problems sizes $n_\ell$, $\ell = 0,\dotsc,L$, are chosen suitably, then the cost of one iteration of MGMC is $\mathcal{O}(n_L)$. Lastly, the autocorrelation between the samples is small and also grid-independent.

\section{\texttt{ParMGMC} -- Parallel Multigrid Monte Carlo}
Having described the general structure of the algorithm, we now discuss our implementation of the MGMC method. We have implemented the algorithm as a a C++ library called \texttt{ParMGMC} available at \url{github.com/nilsfriess/ParMGMC}.
The library is built on top of the Portable and Extensible Toolkit for Scientific computing (PETSc)~\cite{petsc-web-page,petsc-user-ref}, a widely-used software package for the parallel solution of partial differential equations and optimisation problems. PETSc provides a number of parallel data structures and algorithms that are useful for the implementation of the MGMC method. In the following we will go over the individual components of the algorithm and discuss their parallel implementation.

\subsection{Matrices, vectors, parallel communication}
For the parallel distribution and assembly of the matrices and generation of compatible vectors we use PETSc's parallel \texttt{Mat} and \texttt{Vec} formats. Currently, the sequential and parallel variants of the \texttt{MATAIJ} format are supported. Other formats, in particular the \texttt{MATBAIJ} formats, which represent block sparse matrices, are not yet supported. Further, the fact that the precision matrix is necessarily symmetric is not yet exploited to improve performance.

PETSc employs the Message Passing Interface (MPI) for parallel communication and provides different abstractions so that direct calls to MPI routines are rarely required. We make use of the \texttt{VecScatter} data structure and associated routines to handle the parallel communication required in the parallel implementation of the Gibbs sampler (see below).

\subsection{Grid hierarchy}
We provide two variants of the MGMC algorithm: the first is based on a grid hierarchy where each grid is implemented using PETSc's \mintinline{cpp}|DMDA| data structure, which is a sub-type of \mintinline{cpp}|DM| (PETSc's abstract base type for grid-like objects), and is used to manage data for a structured grid in up to three dimensions. We provide a class \mintinline{cpp}|DMHierarchy| which has a constructor
\begin{minted}{cpp}
  DMHierarchy(DM coarseSpace, PetscInt nLevels)
\end{minted}
that builds the grid hierarchy using PETSc's \texttt{DMRefine} routine. The constructed \mintinline{cpp}|DMHierarchy| object can then be passed to the constructor of the MGMC sampler together with the precision matrix on the finest level. The constructor has the following signature.
\begin{minted}{cpp}
  MultigridSampler(const std::shared_ptr<LinearOperator> &fineOperator,
                   const DMHierarchy &dmHierarchy,
                   Engine &engine,
                   const MGMCParameters &params)
\end{minted}
The class \mintinline{cpp}|LinearOperator| is a wrapper around PETSc's \mintinline{cpp}|Mat| class that also stores some additional information about the matrix, in particular the colouring (see below).

The parameter \mintinline{cpp}|engine| is a reference to a random number engine. Its type \mintinline{cpp}|Engine| is templated so that different engines can be tested easily. In all instances where random numbers are generated within the class, this engine is used. This ensures that a global random number generator can be defined in the main function and then be passed around so that a single seed is sufficient to determine the randomness of the program.

The last argument can be used to pass additional parameters. The type \mintinline{cpp}|MGMCParameters| has the following structure
\begin{minted}{cpp}
  struct MGMCParameters {
    PetscInt nSmooth;
    MGMCSmoothingType smoothingType;
    MGMCCycleType cycleType;
    MGMCCoarseSamplerType coarseSamplerType;
  };
\end{minted}
The first member variable \mintinline{cpp}|nSmooth| specifies the number of iterations of the random smoothers on each level (the given value is used both for the pre- and the post-smoothers). The next member determines the type of Gibbs sweeps that are performed. Possible values are \mintinline{cpp}|MGMCSmoothingType::ForwardBackward| and \mintinline{cpp}|MGMCSmoothingType::Symmetric|. The first option means that forward sweeps are performed for the pre-smoothers while backward sweeps are performed for the post-smoothers. The second option means that all random smoothers are using symmetric sweeps.

The value of \mintinline{cpp}|cycleType| indicates whether a V-cycle (\mintinline{cpp}|MGMCCycleType::V|) or a W-cycle (\mintinline{cpp}|MGMCCycleType::W|) should be used. The purpose of the last member variable is explained in the next section. 

The second variant of the MGMC sampler allows it to be used with external libraries and more complicated, possibly unstructured grids. In this case, a new sampler class has to be defined which is derived from the \mintinline{cpp}|MultigridSampler| class:
\begin{minted}{cpp}
  template <class Engine> 
  struct ExampleSampler : MultigridSampler<Engine> {
    ExampleSampler(...)
      : MultigridSampler<Engine>(params, levels, engine) {
      ...
      for (int l = 0; l < levels; ++l)
        this->ops[l] = std::make_shared<parmgmc::LinearOperator>(...);
      ...
    }

    PetscErrorCode restrict(std::size_t level, Vec residual, 
                            Vec f) override { ... }

    PetscErrorCode prolongateAdd(std::size_t level, Vec coarse,
                                 Vec fine) override { ... }
  };    
\end{minted}
The constructor of this new class has to call the constructor of the base class with the desired parameters, the number of levels and the random number engine. It must also define the operators for each level as depicted in the for loop. Further, since there is no \texttt{DMHierarchy} any more, the class has to override the methods \texttt{restrict} and \mintinline{cpp}|prolongateAdd| which correspond to lines 8 and 13 of Algorithm~\ref{alg:mgmc}, respectively.

\subsection{Coarse grid sampler}
The original paper~\cite{goodmansokal} uses Gibbs samplers on all grids, including the coarsest. As in classic multigrid, it is also possible to use a direct method on the coarsest grid (if the problem size there is sufficiently small). We have implemented both approaches, selectable via the the \mintinline{cpp}|MGMCCoarseSamplerType| parameter in the \mintinline{cpp}|MGMCParameters| class (see above). Possible values are \mintinline{cpp}|MGMCCoarseSamplerType::Standard| (which uses to $2 \times \texttt{nSmooth}$ symmetric Gibbs sweeps on the coarsest level) and \mintinline{cpp}|MGMCCoarseSamplerType::Cholesky|.

 In the case where a Cholesky sampler is used on the coarsest level, we rely on the PARDISO implementation of the Intel Math Kernel Library (MKL)~\cite{intelmkl} which provides both a serial and a parallel implementation of sparse Cholesky factorisation and the possibility to perform the forward substitution and the backward substitution step separately (we found that most other parallel Cholesky implementations lack this option). We access the Intel MKL library through an abstraction layer provided by PETSc.\footnote{At the time of writing, not all options of the Intel MKL PARDISO implementation are accessible from PETSc. The option to perform the forward and backward substitutions individually was only recently contributed to PETSc by the author and is not yet included in a released PETSc version.} 

\subsection{Gibbs samplers (random smoothers)}
Recall the component-wise form of the Gibbs sweep
\begin{equation}
    y^{(k+1)}_j = - \frac{1}{a_{jj}} \left(
        \sum_{m=1}^{j-1} a_{mj} y^{(k+1)}_m + \sum_{m=j+1}^n a_{mj} y^{(k)}_m
      \right) + \frac{1}{a_{jj}} c^{(k)}_j \,,\quad \text{for $j=1,\dotsc,n$.}
      \label{eq:gibbs_comp2}
\end{equation}

To parallelise~\eqref{eq:gibbs_comp2}, we exploit to sparse structure of $A$. Let us first consider a simple example and then extend the idea below. Consider the grid in Fig.~\ref{fig:lattice1} and define the precision matrix $A \in \mathbb{R}^{25 \times 25}$ as
\begin{equation*}
   a_{ij} = 
   10^{-4} \delta_{ij} + 
    \begin{cases}
        4 & \text{if $i = j$,} \\
        -1 &  \text{if $i \neq j$ and vertex $i$ and vertex $j$ are neighbours,} \\
        0 & \text{otherwise,}
    \end{cases}
\end{equation*} 
Let $y^{(k)}$ be a sample on the grid, flattened into a vector in $\mathbb{R}^{25}$. The key observation is that one can change the order in which the components in~\eqref{eq:gibbs_comp2} are updated and partition the indices into two sets such that the components within the sets can be updated in parallel. Consider for instance the grid point $7$ in Fig.~\ref{fig:lattice1}. To update its value only the value of its direct neighbours is required due to the local structure of the precision matrix. Collecting all vertices that can be updated independently of each other in separate sets, one obtains a vertex colouring of the graph. In this case, two colours (traditionally, red and black) are sufficient as depicted in Fig.~\ref{fig:lattice2}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{.48\linewidth}
        \centering
        \begin{tikzpicture}[scale=1.3]
            \draw (0,0) grid (4,4);

            \foreach \x in {0,...,4}{
                \foreach \y in {0,...,4}{
                    \pgfmathsetmacro\result{int(5 * \y + \x + 1)}
                    \draw[fill=white] (\x,\y) circle (.5ex);
                    \draw (\x,\y) node[anchor=north east] {$\result$};
                }
            }
            
            \draw[thick,red] (1,1) -- +(0,1);
            \draw[thick,red] (1,1) -- +(1,0);
            \draw[thick,red] (1,1) -- +(-1,0);
            \draw[thick,red] (1,1) -- +(0,-1);
            
            \draw[fill=red] (1,1) circle (0.6ex);

            \draw[fill=black] (1,1)+(0,1) circle (0.5ex);
            \draw[fill=black] (1,1)+(1,0) circle (0.5ex);
            \draw[fill=black] (1,1)+(0,-1) circle (0.5ex);
            \draw[fill=black] (1,1)+(-1,0) circle (0.5ex);
        \end{tikzpicture}
        \caption{The node with index $7$ only depends on its direct neighbours.}\label{fig:lattice1}
    \end{subfigure}\hfill%
    \begin{subfigure}[t]{.48\linewidth}
        \centering
        \begin{tikzpicture}[scale=1.3]
            \draw (0,0) grid (4,4);
            \draw[fill=red] (0,0) circle (.5ex);
            \draw[fill=red] (0,2) circle (.5ex);
            \draw[fill=red] (0,4) circle (.5ex);
            \draw[fill=red] (1,1) circle (.5ex);
            \draw[fill=red] (1,3) circle (.5ex);
            \draw[fill=red] (2,0) circle (.5ex);
            \draw[fill=red] (2,2) circle (.5ex);
            \draw[fill=red] (2,4) circle (.5ex);
            \draw[fill=red] (3,1) circle (.5ex);
            \draw[fill=red] (3,3) circle (.5ex);
            \draw[fill=red] (4,0) circle (.5ex);
            \draw[fill=red] (4,2) circle (.5ex);
            \draw[fill=red] (4,4) circle (.5ex);

            \newcommand\Square[1]{+(-#1,-#1) rectangle +(#1,#1)}
            \draw[fill=black] (0,1) \Square{0.5ex};
            \draw[fill=black] (0,3) \Square{0.5ex};
            \draw[fill=black] (1,0) \Square{0.5ex};
            \draw[fill=black] (1,2) \Square{0.5ex};
            \draw[fill=black] (1,4) \Square{0.5ex};
            \draw[fill=black] (2,1) \Square{0.5ex};
            \draw[fill=black] (2,3) \Square{0.5ex};
            \draw[fill=black] (3,0) \Square{0.5ex};
            \draw[fill=black] (3,2) \Square{0.5ex};
            \draw[fill=black] (3,4) \Square{0.5ex};
            \draw[fill=black] (4,1) \Square{0.5ex};
            \draw[fill=black] (4,3) \Square{0.5ex};

            \draw (0,0) node[anchor=north east] {\phantom{$1$}};
        \end{tikzpicture}
        \caption{Extending the idea creates a red/black colouring of the lattice.}\label{fig:lattice2}
    \end{subfigure}
    \caption{A two-dimensional $5 \times 5$ lattice.}\label{fig:lattice}
  \end{figure}

If the data dependencies between nodes are more complex (e.g., when the local neighbourhood that influences a node is bigger, or when the problem is not posed on a simple grid but an unstructured mesh), more colours will be necessary to colour the nodes such that no two nodes of the same colour depend on each other. Note that due to~\eqref{eq:galerkin}, the number of colours that are necessary to colour the graph of a coarse level matrix $A_{\ell-1}$ can generally be bigger than the number of colours required for $A_{\ell}$.

This allows to parallelise the method in a shared-memory context. If the problem is scattered on different processes that do not share common memory, communication between the processes is necessary. Suppose the grid is distributed among four processes as depicted in Fig.~\ref{fig:latticedist} (we labelled the vertices using PETSc's standard ordering). A distributed-memory parallel Gibbs sampler can then be defined as follows:
\begin{enumerate}
    \item Send/receive values at the black boundary vertices.
    \item Perform the Gibbs sweep defined by~\eqref{eq:gibbs_comp} for the red vertices.
    \item Send/receive values at the red boundary vertices.
    \item Perform the Gibbs sweep defined by~\eqref{eq:gibbs_comp} for the black vertices.
\end{enumerate}
The extension to colourings with more than two colours is straightforward: First send/receive the values that the nodes of the current colour depend on, then update those nodes, and proceed with the next colour.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=1.3]
        \draw (0,0) grid (4,4);
        \draw[fill=red] (0,0) circle (.5ex);
        \draw[fill=red] (0,2) circle (.5ex);
        \draw[fill=red] (0,4) circle (.5ex);
        \draw[fill=red] (1,1) circle (.5ex);
        \draw[fill=red] (1,3) circle (.5ex);
        \draw[fill=red] (2,0) circle (.5ex);
        \draw[fill=red] (2,2) circle (.5ex);
        \draw[fill=red] (2,4) circle (.5ex);
        \draw[fill=red] (3,1) circle (.5ex);
        \draw[fill=red] (3,3) circle (.5ex);
        \draw[fill=red] (4,0) circle (.5ex);
        \draw[fill=red] (4,2) circle (.5ex);
        \draw[fill=red] (4,4) circle (.5ex);

        \newcommand\Square[1]{+(-#1,-#1) rectangle +(#1,#1)}
        \draw[fill=black] (0,1) \Square{0.5ex};
        \draw[fill=black] (0,3) \Square{0.5ex};
        \draw[fill=black] (1,0) \Square{0.5ex};
        \draw[fill=black] (1,2) \Square{0.5ex};
        \draw[fill=black] (1,4) \Square{0.5ex};
        \draw[fill=black] (2,1) \Square{0.5ex};
        \draw[fill=black] (2,3) \Square{0.5ex};
        \draw[fill=black] (3,0) \Square{0.5ex};
        \draw[fill=black] (3,2) \Square{0.5ex};
        \draw[fill=black] (3,4) \Square{0.5ex};
        \draw[fill=black] (4,1) \Square{0.5ex};
        \draw[fill=black] (4,3) \Square{0.5ex};

        \draw[dashed] (-0.6,-0.5) -- (2.4, -0.5) -- (2.4,2.4) -- (-0.6,2.4) -- cycle;
        \draw[dashed] (2.6,-0.5) -- (4.6, -0.5) -- (4.6,2.4) -- (2.6,2.4) -- cycle;
        \draw[dashed] (-0.6,2.6) -- (-0.6, 4.6) -- (2.4,4.6) -- (2.4,2.6) -- cycle;
        \draw[dashed] (2.6,2.6) -- (4.6, 2.6) -- (4.6,4.6) -- (2.6,4.6) -- cycle;

        \draw (-0.6, 1) node[anchor=east] {$P1$};
        \draw (4.6, 1) node[anchor=west] {$P2$};
        \draw (-0.6, 3.5) node[anchor=east] {$P3$};
        \draw (4.6, 3.5) node[anchor=west] {$P4$};

        \draw (0,0) node[anchor=north east] {$1$};
        \draw (1,0) node[anchor=north east] {$2$};
        \draw (2,0) node[anchor=north east] {$3$};
        \draw (0,1) node[anchor=north east] {$4$};
        \draw (1,1) node[anchor=north east] {$5$};
        \draw (2,1) node[anchor=north east] {$6$};
        \draw (0,2) node[anchor=north east] {$7$};
        \draw (1,2) node[anchor=north east] {$8$};
        \draw (2,2) node[anchor=north east] {$9$};

        \draw (3,0) node[anchor=north west] {${10}$};
        \draw (4,0) node[anchor=north west] {${11}$};
        \draw (3,1) node[anchor=north west] {${12}$};
        \draw (4,1) node[anchor=north west] {${13}$};
        \draw (3,2) node[anchor=north west] {${14}$};
        \draw (4,2) node[anchor=north west] {${15}$};

        \draw (0,3) node[anchor=south east] {${16}$};
        \draw (1,3) node[anchor=south east] {${17}$};
        \draw (2,3) node[anchor=south east] {${18}$};
        \draw (0,4) node[anchor=south east] {${19}$};
        \draw (1,4) node[anchor=south east] {${20}$};
        \draw (2,4) node[anchor=south east] {${21}$};

        \draw (3,3) node[anchor=south west] {${22}$};
        \draw (4,3) node[anchor=south west] {${23}$};
        \draw (3,4) node[anchor=south west] {${24}$};
        \draw (4,4) node[anchor=south west] {${25}$};
    \end{tikzpicture}
    \caption{The two-dimensional lattice distributed on four processes using PETSc's default global indexing scheme.}\label{fig:latticedist}
\end{figure}

The actual colouring of the matrix is handled in the \texttt{LinearOperator} class which provides a method \texttt{colorMatrix()} (which generates a general colouring using PETSc's \texttt{MatColoring} routines), and an overloaded variant \texttt{colorMatrix(DM)} which generates a red/black colouring compatible with the given \texttt{DM}.

As mentioned above, we use PETSc's \mintinline{cpp}|VecScatter| data structures to handle the parallel communication. For each colour, a \mintinline{cpp}|VecScatter| context is defined and the send/receive steps above are then implemented as
\begin{minted}{cpp}
  auto scatter = linearOperator->getColoring()->getScatter(i);
  auto ghostvec = linearOperator->getColoring()->getGhostVec(i);

  VecScatterBegin(scatter, sample, ghostvec, 
                  INSERT_VALUES, SCATTER_FORWARD);
  VecScatterEnd(scatter, sample, ghostvec, 
                INSERT_VALUES, SCATTER_FORWARD);
\end{minted}
This scatters the values of \mintinline{cpp}|sample| at boundary nodes on remote processes into the local vector \mintinline{cpp}|ghostVec| that is then used during the Gibbs sweep.

A multicolouring approach is not the only option to parallelise a Gibbs sampler. Due to the structural similarity of the Gibbs sampler to the Gauss-Seidel method, parallelisation strategies for Gauss-Seidel methods can directly be applied to a parallel Gibbs sampler implementation. One approach that works directly on the level of the matrix graph is presented in~\cite{adams2001}, and it is demonstrated that it outperforms multicolour Gauss-Seidel in cases where many colours are necessary to partition the nodes appropriately. Instead of colouring the vertices, the algorithm first introduces an ordering between the processes. Then, boundary nodes are partitioned into three sets: the \emph{top} nodes (which only communicate with lower processes), the \emph{bot} nodes (which only communicate with higher processes), and the \emph{mid} nodes (the remaining boundary nodes). The algorithm then proceeds by first updating top, then mid, then bot nodes. Parallel communication of the updated nodes is overlapped with the computation for the interior nodes, and only when updating the mid nodes it might be necessary to wait for other processes to finish first. We have not implemented this method yet, but plan to do so in a future version.

Other parallelisation strategies for Gauss-Seidel exploit that it is typically not used as a standalone solver but only as a smoother in multigrid. In this case, local Gauss-Seidel sweeps are performed independently on each process, communicating boundary values only after each full sweep (i.e., data dependencies between nodes across processor boundaries are ignored during the sweeps). When used as a solver, this approach might not converge to the correct solution but it often works sufficiently well when used as a multigrid smoother~\cite{parmultigrid}. The equivalent Gibbs sampler, usually referred to as \emph{Hogwild} Gibbs~\cite{hogwild}, has also been studied in the literature, although the generated samples do not converge to the target distribution. Our library also provides a class \mintinline{cpp}|HogwildGibbsSampler| which implements this sampler using PETSc's parallel Gauss-Seidel implementation. We use this sampler in some of the numerical experiments as a baseline to measure the overhead of the parallel communication due to the colouring.

\section{Numerical results}\label{sec:numerics}
In this section we study the performance of our implementation. Let us begin with a description of the random field. The precision matrix is a discretisation (using either finite differences or finite elements, see below) of a shifted Laplace operator
\begin{equation}
    \label{eq:shiftedlaplace}
    \mathcal{A} u \coloneqq (\kappa^2 - \Delta) u \qquad \text{for some } \kappa > 0 \,.
\end{equation}
The reason for this specific choice of precision operator is due to a result by Whittle~\cite{whittle1954,whittle1963} (see also~\cite{lindgren,lindgrenSPDEApproachGaussian2022}). Consider the stochastic partial differential equation (SPDE) $\tau (\kappa^2 - \Delta)^{\alpha/2} u = \mathcal{W}$, where $\tau$ is a normalisation parameter that we ignore in the following, and $\mathcal{W}$ is spatial white noise. The stationary solutions of this SPDE have Mat\'ern covariance function with smoothness parameter $\nu = \alpha - d/2$ where $d$ is the dimension of the domain. The associated precision operator is $\mathcal{A} \coloneqq {(\kappa^2 - \Delta)}^\alpha$~\cite[Sec.\ 2.2]{lindgrenSPDEApproachGaussian2022}, and we see that~\eqref{eq:shiftedlaplace} corresponds to $\alpha = 1$. The smoothness parameters of the associated Mat\'ern fields are then $\nu = 0$ in 2D and $\nu = -\frac{1}{2}$ in 3D, respectively.

We consider two variants that differ in the way~\eqref{eq:shiftedlaplace} is discretised on the finest level to obtain the precision matrix $A_L$. In the first, the operator~\eqref{eq:shiftedlaplace} is discretised on the unit square ${[0,1]}^2$ using a five-point finite difference stencil. In the second, we instead discretise~\eqref{eq:shiftedlaplace} on two unstructured grids using $H^1$ conforming finite elements using the MFEM library~\cite{mfem,mfem-web}. Both cases use homogenous Dirichlet boundary conditions, and the mean $f$ is set to zero. All examples use MGMC configured to run a V-cycle and use two iterations of the random smoothers.

The purpose of the following examples is not to study the statistical properties of the samples generated by MGMC or to compare the performance to other state-of-the-art samplers. Here we focus on the parallel performance of our implementation with particular regard to the overhead caused by the colouring, and the effect of the coarse grid sampler.

\subsection{Structured grid}
We begin with a study of the strong scaling behaviour. To this end, we set up a grid hierarchy with five levels, the coarsest level being of size $257 \times 257$ and the finest level consisting of $4097 \times 4097$ grid points. Since we use a simple 5-point finite difference stencil to discretise~\eqref{eq:shiftedlaplace}, two colours are sufficient to colour the lattice on the finest level. On the remaining levels, the colouring routines implemented in PETSc produced colourings with nine ($\ell = 3$), eight ($\ell = 2$), eight ($\ell = 1$), and nine colours ($\ell = 0$), respectively.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/strong_scaling_hogwild_4097x4097.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/strong_scaling_gibbs_4097x4097.pdf}
      \end{subfigure}
      \caption{Strong scaling speed-up for the \emph{Hogwild} Gibbs sampler (left) and the multicolour Gibbs sampler (right). The multicolour Gibbs sampler uses two colours in this case.}
    \label{fig:2d_structured_ss}
\end{figure}

We begin with a comparison of the \emph{Hogwild} Gibbs sampler and the multicolour Gibbs sampler used as standalone samplers on the finest level. In Fig.~\ref{fig:2d_structured_ss} we plot the strong scaling speed-up $S(p)$ defined as
\begin{equation*}
    S(p) = \frac{T(1)}{T(p)}\,,
\end{equation*}
where $T(p)$ is the execution time (the time to set up the sampler plus the time to generate a fixed number of samples) on $p$ cores, and in Table~\ref{tab:2d_structured_ss} we provide the strong scaling efficiency which is given by
\begin{equation*}
  \text{E(p)} = \frac{S(p)}{p}\,.
\end{equation*}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/strong_scaling_mgmc+hw+gibbs_4097x4097.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/strong_scaling_mgmc+mg+gibbs_4097x4097.pdf}
    \end{subfigure}
    \caption{Strong scaling speed-up for the MGMC sampler using \emph{Hogwild} Gibbs as the random smoothers (left) and multicolour Gibbs as the random smoothers (right). Both use the respective variant of the Gibbs sampler also on the coarsest level.}
    \label{fig:2d_structured_mgmc_ss}
\end{figure}

\begin{table}[htbp]
    \centering
    \begin{tabular}{rccccc}
    \toprule
    \# Proc. & Fig.\ref{fig:2d_structured_ss} (right) & Fig.\ref{fig:2d_structured_ss} (left)  & Fig.\ref{fig:2d_structured_mgmc_ss} (right) & Fig.\ref{fig:2d_structured_mgmc_ss} (left) & Fig.\ref{fig:2d_structured_mgmc_chol_ss} \\
    \midrule
    {2}     &  94\% &  95\% &  89\% &  91\% & 102\% \\
    {4}     &  90\% &  87\% &  84\% &  81\% &  96\% \\
    {8}     &  86\% &  78\% &  87\% &  76\% &  80\% \\
    {16}    &  80\% &  75\% &  78\% &  61\% &  81\% \\
    {32}    &  89\% &  75\% &  79\% &  61\% &  80\% \\
    {64}    &  86\% &  80\% &  76\% &  61\% &  72\% \\
    {128}   &  86\% &  87\% &  78\% &  62\% &  43\% \\
    {256}   &  86\% &  95\% &  78\% &  62\% &  15\% \\
    {512}   &  82\% &  81\% &  75\% &  66\% &  22\% \\
    {1024}  &  68\% &  74\% &  68\% &  63\% &   5\% \\
    \bottomrule
    \end{tabular}
    \caption{Strong scaling efficiency for the 2D structured grid example.
    }\label{tab:2d_structured_ss}
\end{table}

We observe that the behaviour of both samplers is very similar up to 512 processes; the performance of the multicolour Gibbs sampler seems to drop for 1024 cores. Comparing the absolute timings, we found that the \emph{Hogwild} sampler is on average about 13\% faster than the multicolour sampler.

The strong scaling behaviour for the full MGMC sampler without coarse Cholesky sampling is similar, see Fig.~\ref{fig:2d_structured_mgmc_ss}. The variant that uses \emph{Hogwild} Gibbs as random smoothers shows worse scaling in this case (see the last column in Table~\ref{tab:2d_structured_ss}) and we do not have an explanation for that. However, by comparing the timings of both variants for a fixed number of processors, we observed that the variant with \emph{Hogwild} random smoothers was on average 55\% faster than the version with multicolour Gibbs smoothers.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{plots/strong_scaling_mgmc+mg+cholesky_4097x4097.pdf}
    \caption{Strong scaling speed-up for the MGMC sampler using multicolour Gibbs as the random smoothers and Cholesky sampling on the coarsest level.}
    \label{fig:2d_structured_mgmc_chol_ss}
\end{figure}

The strong scaling behaviour changes if we use a Cholesky sampler on the coarsest level, see Fig.~\ref{fig:2d_structured_mgmc_chol_ss}. For this example we only considered MGMC with multicolour Gibbs random smoothers. We see that the performance deteriorates for more than 64 cores and the total runtime on 1024 processors is almost the same as for 128 processors. It should be noted that the Intel MKL Cholesky implementation offers some additional configuration options that could improve the performance (e.g., thread parallelisation within a processor) but we did not test the effect of these options yet.

To conclude this example, we look at the weak scaling behaviour of the MGMC sampler. Fig.~\ref{fig:2d_structured_ws} shows the weak scaling efficiency for the multicolour Gibbs sampler, the MGMC sampler with Gibbs sweeps on the coarsest level, and MGMC using a coarse Cholesky sampler. The random smoother is a multicolour Gibbs sampler in both cases. We have fixed the local problem size of the coarsest level to $64 \times 64$ for all runs.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{plots/weak_scaling_64x64.pdf}
    \caption{Weak scaling efficiency for the multicolour Gibbs sampler and the MGMC sampler, using both Gibbs sampling and Cholesky sampling on the coarsest level. The random smoother is a multicolour Gibbs sampler in both cases.}
    \label{fig:2d_structured_ws}
\end{figure}

While both the standalone Gibbs sampler and the MGMC sampler with coarse Gibbs sampling show acceptable weak scaling performance, the use of the Cholesky sampler on the coarsest level seems to drastically reduce the efficiency. We note here again that this behaviour could be mitigated by adjusting the parameters in the Intel MKL library.

\subsection{Unstructured Grids}
In this section we consider two examples that discretise~\eqref{eq:shiftedlaplace} on unstructured grids using finite elements. The two grids are shown in Fig.~\ref{fig:mfem_ex}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/paraview/square-disc_0_05.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/paraview/toroid_1.png}
      \end{subfigure}
      \caption{Random samples on the grids discretised using finite elements. Both have precision operator $\kappa^2 - \Delta $, the square disc (left) uses $\kappa^{-1} = 0.05$, the twisted toroid (right) uses $\kappa^{-1} = 1$.}
    \label{fig:mfem_ex}
\end{figure}

We begin with a weak scaling test for the square disc example. In this case, instead of fixing the number of grids in the hierarchy, as well as the local coarse grid size, we fix the global coarse grid size and increase the number of levels with increasing number of cores. The number of levels ranges from three (for the one processor run) up to eight (for the run on 1024 processes). On the coarse grid we have $79616$ unknowns, on the finest grid there are approximately $1.2 \times 10^9$.

The number of colours required for the matrix colouring are (from coarse to fine)
\begin{equation*}
  7,\ 15,\ 18,\ 19,\ 19,\ 20,\ 20,\ 19\,.
\end{equation*}
These numbers differ slightly depending on the number of processors.

A plot of the weak scaling efficiency for the square disc example is given in Fig.~\ref{fig:2d_unstructured_ws}. We observe that neither sampler scales optimally, in particular when comparing the behaviour on four and sixteen cores to the single processor run. We also observe that unlike in the previous section, using a Cholesky sampler on the coarsest level does not affect the weak scaling performance much, indicating that the additional communication due to the colouring dominates the cost. Comparing absolute timings shows that the costs per sample of both variants of the MGMC sampler with multicolour Gibbs sweeps as random smoothers are within 10\% of each other for up to 64 processes; after that the coarse Cholesky sampler becomes more expensive.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{plots/ws_mgmc_fe.pdf}
    \caption{Weak scaling efficiency for the  MGMC sampler applied to the square disc example.}
    \label{fig:2d_unstructured_ws}
\end{figure}

We continue to explore the effect of the coarse grid samplers by performing a strong scaling test for the toroid example. The grid hierarchy now consists of five levels (ranging from $2{\small,}160$ to $6{\small,}439{\small,}680$ unknowns) and the number of colours required to colour the matrix graphs are (from coarse to fine): $16,45,59,63$. Again, these values depend on the number of processors that are used (the run with 1024 processes, for instance, required $16,47,61$ and $72$ colours, respectively). 

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/strong_scaling_mgmc+mg+cholesky_toroid.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/strong_scaling_mgmc+mg+gibbs_toroid.pdf}
      \end{subfigure}
      \caption{Strong scaling speed-up for the toroid example. On the left, we used the MGMC sampler with Cholesky sampling on the coarsest grid, the right plot shows the results for using a multicolour Gibbs sampler instead. The random smoothers on the remaining levels are multicolour Gibbs samplers in both cases.}
    \label{fig:toroid_ss}
\end{figure}

The results are shown in Fig.~\ref{fig:toroid_ss}. Here, we again observe that the variant that uses a Cholesky sampler on the coarsest level scales worse than the sampler that uses multicolour Gibbs sweeps on the coarsest level. We also observe that the high number of colours on the coarser levels still allow for acceptable scaling, at least up to around 128 cores. Finally, we compare the absolute timings for this example. In Fig~\ref{fig:timings} we plotted the runtime for the two variants of the MGMC sampler. We report the time to set up the sampler and the time to generate 500 samples separately (the time to set up the finite element mesh hierarchy using the MFEM library is not included). We see that both sampler variants perform very similar up to 64 processes; after that, the sampling time for the Cholesky sampler stays approximately constant.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{plots/runtime.pdf}
    \caption{Comparison of the timings for the strong scaling example on the toroid mesh. The timings to set up the MGMC sampler and to generate the samples are reported separately. The left bars show the timings for the variant using a Gibbs sampler on the coarsest level, the right bars show timings using a coarse Cholesky sampler.}
    \label{fig:timings}
\end{figure}

\section{Conclusion}
In this report we have presented a parallel implementation of the Multigrid Monte Carlo method in PETSc. We have given a brief theoretical introduction to the method and an overview of the parallel implementation of the individual components of the sampler.

We then presented three numerical examples to study the scaling properties of the implementation, with particular focus on the effect of the colouring and the difference between the two coarse sampler types. It was demonstrated that the implementation shows acceptable strong and weak scaling if a simple red/black colouring can be used on the finest level, and Gibbs sweeps are used on the coarsest level. Using a Cholesky sampler reduced the performance.

The examples posed on unstructured grids both showed worse scaling due to the higher number of colours that are necessary to colour the matrix graph. For the two-dimensional example, neither sampler variant showed good weak scaling, and the performance was not affected much by using a coarse Cholesky sampler. For the three-dimensional example, we observed similar runtimes and strong scaling behaviour on up to $\sim 128$ processors; after that, the sampling cost of the Cholesky sampler variant does not decrease with increasing number of processors.

We also saw that if the number of colours is sufficiently low, using a Cholesky sampler on the coarsest level adversely affects the scalability. As noted above, this could be improved by introducing thread-parallelism on the level of individual cores, but it is well-known that multigrid scalability generally suffers from the non-optimal algorithmic complexity of direct coarse solvers (or samplers, in our case). A common strategy to improve upon this issue is to reduce the number of processors that participate on the coarser levels (e.g., the finest level uses $64$ processors, the next level $16$, and the coarsest only one). This is sometimes referred to as agglomeration and is implemented in PETSc for some grid configurations~\cite{petsc-multigrid}. We plan to implement this strategy in a future version.

As we have seen in the examples, the number of colours typically grows from the finest to the coarsest level. This is due to definition of the coarse-level matrices by Galerkin projection of the finer level matrix. In geometric multigrid, another option to define the coarse-level problems is to rediscretise the operator on each level. In the two-dimensional structured grid example, this would mean that a red/black colouring would suffice on all levels, which could potentially reduce the overhead due to the communication of the boundary nodes. However, the convergence theory of~\cite{kazashimuellerscheichl} currently does not cover the case of precision matrices obtained by rediscretisations, and it is not known if the theory can be extended in this regard.

Another aspect that we have not covered in this report is the effect of different random number generators. In our examples, the generation of random numbers was responsible for up to 20\% of the application's runtime. We used the permuted congruential generator (PCG)~\cite{oneill:pcg2014} random number generator, but other algorithms could be a possibility to increase the performance. Note, that the high cost of the random number generation could also be reduced by the agglomeration strategy explained above. As some processors are inevitably idle on the coarser levels, they can already generate random numbers in the meantime.

\printbibliography
\end{document}

%%% Local Variables: 
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
