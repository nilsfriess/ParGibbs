\documentclass[
fontsize=11pt,
paper=a4,
numbers=noenddot
]{scrartcl}

\usepackage[bottom=3cm, top=3cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}

\usepackage[british]{babel}

\usepackage{csquotes}

\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}

\usepackage{hyperref}  

\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{todonotes}
\usepackage[ruled,vlined,resetcount]{algorithm2e}
\SetKwInput{KwData}{Input}


\addtokomafont{disposition}{\rmfamily}

\title{Parallel Multigrid Monte Carlo\\
{\normalsize Software practical report}}
\date{\today}
\author{Nils Friess}

\begin{document} 
\maketitle

\begin{abstract}
    This report presents a parallel implementation of the Multigrid Monte Carlo (MGMC) method for sampling from Gaussian distributions with large and sparse precision matrix on distributed memory machines. MGMC is in a certain sense the stochastic counterpart of the classic geometric Multigrid method where the smoothers are replaced by certain random samplers. The implementation is based on the \emph{Portable and Extensible Toolkit for Scientific computing (PETSc)} and offers three parallel Gibbs-type samplers that can be used within MGMC. First, a multicolour Gibbs sampler which partitions the rows of the precision matrix into independent sets that can be handled in parallel; second, a parallel Gibbs sampler that partitions the matrix graph into interior and boundary nodes to increase the parallel efficiency; and third, a so-called \emph{Hogwild} Gibbs sampler, which does not target the desired distribution but offers a higher degree of parallelisation. We provide several numerical examples to study both the general characteristics of the MGMC method and the scaling of the parallel implementation.
\end{abstract}

\section{Introduction}
Efficiently generating samples from high-dimensional Gaussian distributions is an important task in many computational sciences but remains computationally challenging when the problem becomes so large that the data does not fit in the memory of a single computing node or if the computation should be sped up by distributing the work among multiple processors or computers. In this report we present a parallel implementation of the \emph{Multigrid Monte Carlo (MGMC)} method~\cite{goodmansokal} to sample from Gaussian distributions with large and sparse precision matrix (inverse covariance matrix). The implementation is based on the \emph{Portable and Extensible  Toolkit for Scientific computing (PETSc)}~\cite{petsc-web-page,petsc-user-ref} which is a widely used library for the solution of partial differential equations and related problems on distributed memory machines. The implementation is available as a free and open source software package at \url{https://github.com/nilsfriess/ParMGMC}.

In the next section we give a brief introduction to the MGMC method, and we give an overview of our implementation. Since the MGMC method is in a certain sense a stochastic version of the classic (geometric) Multigrid algorithm, large parts of the implementation can reuse parallel data structures and algorithms provided by PETSc (e.g., parallel matrix formats, or the parallel assembly and application of grid-transfer operators) so that we only briefly comment on those aspects of the implementation. Our focus is the parallelisation of the random samplers that replace the smoothers in the classic Multgrid method which we present in the third section. The implementation offers three Gibbs-type random samplers: a multicolour Gibbs sampler, a parallel Gibbs sampler based on a distributed-memory Gauss-Seidel solver~\cite{adams2001}, and a so-called \emph{Hogwild} Gibbs sampler. In the fourth section we present numerical examples to study the general characteristics of the MGMC method and the scaling of the parallel implementation. We conclude the report with some remarks on possible future work.

\section{Multigrid Monte Carlo}
MGMC was first introduced in the context of simulating lattice field theories as a means to reduce or even eliminate the phenomenon of \emph{critical slowing down}: For classical algorithms, such as the \emph{heat bath} algorithm, the autocorrelation time $\tau$ (the time that is necessary to obtain a new useful grid configuration during the simulation) typically behaves as $\tau \sim h^z$ where $h$ is the grid spacing and usually $z \approx 2$. Thus, as the continuum limit $h \rightarrow 0$ is approached, the computer time increases drastically, rendering these algorithms practically useless. As already described in the original paper~\cite{goodmansokal}, the same holds true when these algorithms are used to generated Gaussian samples (since this simply corresponds to a specific type of Hamiltonian operator that is used to describe the physical theory). In this case, the \emph{heat bath} algorithm is a Gibbs sampler and the autocorrelation time roughly corresponds to the number of samples that need to be generated to obtain one statistically independent sample.

In essence, the MGMC algorithm corresponds to the classic (geometric) multigrid algorithm~\cite{hackbuschMultiGridMethodsApplications1985}, whereby the \emph{pre- and postsmoothers} are replaced by \emph{pre- and postsamplers} (which we also refer to as random smoothers to emphasise the connection to classic Multigrid) and the direct solver on the coarsest level is replaced by a random sampler. More precisely, for every Multigrid method with a smoother based on a convergent matrix splitting there exists a corresponding MGMC method (if we fix the coarse sampler). This correspondence follows from a general equivalence between stationary iterative methods and so called generalised Gibbs samplers~\cite{foxparker}. In the case of the Gauss-Seidel method, for example, the corresponding sampler is the classical component-wise Gibbs sampler.

The remaining components of the Multigrid method (in particular, the prolongation and restriction operations) are used in the same way in the MGMC method. An implementation can thus reuse parts of a given Multigrid implementation and for our parallel implementation we can leverage PETSc's optimised parallel implementations of these components. For the coarse sampler, we either use a few iterations of a Gibbs sampler or we generate the samples using a Cholesky factorisation of the coarse matrix. We used the 

\section{Parallel Gibbs sampling}

\section{Numerical results}

\section{Conclusion}

\section{Notes}
\subsection{Cholesky sampling from Gaussian distributions with given precision matrix}
Suppose we want to generate samples from the distribution $\pi = \mathcal{N}(A^{-1} f, A^{-1})$. Let $LL^T = A$ be a Cholesky decomposition of the precision matrix $A$. Samples from $\pi$ can then be generated as follows:
\begin{enumerate}
    \item Draw a sample $\xi \sim \mathcal{N}(0,I)$.
    \item Forward-solve $L g = f$ for $g$.
    \item Backward-solve $L^T x = \xi + g$.
    \item Return the new sample $x$.
\end{enumerate}
Indeed, we then have $x = L^{-T} \xi + L^{-T} g = L^{-T} \xi + {(LL^T)}^{-1} f =  L^{-T} \xi + A^{-1} f$ and by the properties of the normal distribution it follows that $x \sim \mathcal{N}(A^{-1} f, L^{-T}L^{-1}) = \mathcal{N}(A^{-1} f, A^{-1})$ as claimed.

\subsection{Finite Difference discretisation of the shifted Laplace problem}
Consider the following PDE
\begin{align*}
    -u_{xx}(x,y) - u_{yy}(x,y) + \kappa^2 u(x,y) &= 0 \qquad \text{for $0 < x,y < 1$} \\
    u(x,y) &= 0 \qquad \text{for $x = 0, x = 1, y = 0, y = 1$.}
\end{align*}
Using a classic 5-point stencil, we obtain the discrete equations
\begin{equation*}
    -\frac{1}{h^2}\left(
        u_{i+1,j} + u_{i,j+1} - 4u_{i,j} + u_{i,j-1} + u_{i-1,j}
    \right)
    + \kappa^2 u_{i,j} = 0\,,
\end{equation*}
for $i,j = 1,\dotsc,n-1$.

\printbibliography

\end{document}